<!doctype html>
<html lang="en">
<head>
  <meta id="viewport" name="viewport" content="width=device-width,
    minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"/>
  <script src="propel_website/notebook.js"></script>
  <link rel="stylesheet" href="normalize.css">
  <link rel="stylesheet" href="skeleton.css">
  <link rel="stylesheet" href="codemirror.css"/>
  <link rel="stylesheet" href="style.css"/>
  <link rel="icon" type="image/png" href="favicon.png">
  <title>Propel Notebook</title>
</head>
<body>
  <div class="container">
    <div id=cells>

<script type="notebook">
import { load, imshow } from "mnist";

datasetTrain = load("train", 256, false);
datasetTrain.next().then(({images, labels}) => {
  console.log("labels:", labels);
  for (let i = 0; i < images.shape[0]; i++) {
    let img = images.slice([i, 0, 0], [1, -1, -1]).squeeze();
    imshow(img);
  }
});
</script>

<script type="notebook">
// Simple MNIST classifier.
// Adapted from
// https://github.com/HIPS/autograd/blob/master/examples/neural_net.py
import { $, OptimizerSGD, Params, Tensor } from "propel";

// Hyperparameters
const learningRate = 0.001;
const momentum = 0.9;
const batchSize = 256;
const layerSizes = [784, 200, 100, 10];
const reg = 0.0001;

// Implements a fully-connected network with ReLU activations.
// Returns logits.
// @param params A list of parameters.
// @param images An (N x 28 x 28) tensor.
function inference(params, images) {
  let inputs = images.cast("float32").div(255).reshape([-1, 28 * 28]);
  let outputs;
  for (let i = 0; i < layerSizes.length - 1; ++i) {
    const m = layerSizes[i];
    const n = layerSizes[i + 1];
    // Initialize or get weights and biases.
    const w = params.randn(`w${i}`, [m, n]);
    const b = params.randn(`b${i}`, [n]);
    outputs = inputs.matmul(w).add(b);
    inputs = outputs.relu();
  }
  return outputs;
}

// Define the training objective using softmax cross entropy loss.
function loss(images, labels, params) {
  const labels1H = labels.cast("int32").oneHot(10);
  const logits = inference(params, images);
  const softmaxLoss = logits.softmaxCE(labels1H).reduceMean();
  return softmaxLoss.add(regLoss(params));
}

// Regularization loss. Computes L2 norm of all the params scaled by reg.
function regLoss(params) {
  let s = $(0);
  params.forEach((p) => {
    s = s.add(p.square().reduceSum());
  });
  return s.mul(reg);
}

async function accuracy(params, dataset, nExamples) {
  let totalCorrect = $(0);
  let seen = 0;
  while (seen < nExamples) {
    const {images, labels} = await dataset.next();
    const logits = inference(params, images);
    const predicted = logits.argmax(1).cast("uint8");
    const a = predicted.equal(labels).cast("float32").reduceSum();
    totalCorrect = totalCorrect.add(a);
    seen += images.shape[0];
  }
  const acc = totalCorrect.div(seen);
  return acc.getData()[0];
}

async function train() {
  const opt = new OptimizerSGD();
  while (opt.steps < 5) {
    console.log("get batch");
    const {images, labels} = await datasetTrain.next();

    console.log("make sgd step");

    // Take a step of SGD. Update the parameters opt.params.
    const l = opt.step(learningRate, momentum, (params) => {
      return loss(images, labels, params);
    });

    console.log("step", opt.steps, "loss", l.toFixed(3));
  }
}
</script>

<script type="notebook">
// Uncomemnt to run training loop.
// train();
</script>

    </div> <!-- #cells -->
    <div class="row">
      <button id="newCell">New Cell</button>
    </div>
  </div> <!-- container -->
</body>
</html>
